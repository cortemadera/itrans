{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "def get_tokenizer(text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    return max(len(x) for x in lines)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    sequences = pad_sequences(sequences, maxlen=length, padding='post')\n",
    "    return sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array\n",
    "def encode_one_hot_sequences(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dense\n",
    "def define_model(src_vocab, tgt_vocab, src_timesteps, tgt_timesteps, n_units):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True),\n",
    "        keras.layers.LSTM(n_units),\n",
    "        keras.layers.RepeatVector(tgt_timesteps),\n",
    "        keras.layers.LSTM(n_units, return_sequences=True),\n",
    "        keras.layers.Dense(tgt_vocab, activation='softmax')\n",
    "    ])\n",
    "    #model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    #model.add(LSTM(n_units))\n",
    "    #model.add(RepeatVector(tgt_timesteps))\n",
    "    #model.add(LSTM(n_units, return_sequence=True))\n",
    "    #model.add(TimeDistributed(Dense(tgt_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "def word_tokenize(text):\n",
    "    result = ''\n",
    "    s = SnowNLP(text)\n",
    "    for w in s.words:\n",
    "        word = w.strip()\n",
    "        if word != '':\n",
    "            result = result + ' ' + word\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def read_data():\n",
    "    pairs = []\n",
    "    DATA_PATH = os.path.join(\"../datasets\", \"cmn.txt\")\n",
    "    file = open(DATA_PATH, mode='r', encoding ='utf8')\n",
    "    text = file.read()\n",
    "    lines = text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        splits = line.split('\\t')\n",
    "        src = line.split('\\t')[0]\n",
    "        tgt = word_tokenize(splits[1])\n",
    "        pair = [src, tgt]\n",
    "        pairs.append(pair)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 28, 256)           879360    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 33, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 33, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 33, 5205)          1337685   \n",
      "=================================================================\n",
      "Total params: 3,267,669\n",
      "Trainable params: 3,267,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "141/141 - 82s - loss: 1.9087 - val_loss: 1.5075\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.50751, saving model to mtrans.h5\n",
      "Epoch 2/100\n",
      "141/141 - 73s - loss: 1.1419 - val_loss: 1.4677\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.50751 to 1.46766, saving model to mtrans.h5\n",
      "Epoch 3/100\n",
      "141/141 - 82s - loss: 1.0996 - val_loss: 1.4360\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.46766 to 1.43602, saving model to mtrans.h5\n",
      "Epoch 4/100\n",
      "141/141 - 124s - loss: 1.0549 - val_loss: 1.4346\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.43602 to 1.43460, saving model to mtrans.h5\n",
      "Epoch 5/100\n",
      "141/141 - 124s - loss: 1.0397 - val_loss: 1.4367\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.43460\n",
      "Epoch 6/100\n",
      "141/141 - 127s - loss: 1.0268 - val_loss: 1.4392\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.43460\n",
      "Epoch 7/100\n",
      "141/141 - 128s - loss: 1.0150 - val_loss: 1.4347\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.43460\n",
      "Epoch 8/100\n",
      "141/141 - 126s - loss: 1.0061 - val_loss: 1.4352\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.43460\n",
      "Epoch 9/100\n",
      "141/141 - 127s - loss: 0.9992 - val_loss: 1.4370\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.43460\n",
      "Epoch 10/100\n",
      "141/141 - 117s - loss: 0.9888 - val_loss: 1.4398\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.43460\n",
      "Epoch 11/100\n",
      "141/141 - 108s - loss: 0.9784 - val_loss: 1.4393\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.43460\n",
      "Epoch 12/100\n",
      "141/141 - 107s - loss: 0.9622 - val_loss: 1.4265\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.43460 to 1.42649, saving model to mtrans.h5\n",
      "Epoch 13/100\n",
      "141/141 - 106s - loss: 0.9433 - val_loss: 1.4209\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.42649 to 1.42090, saving model to mtrans.h5\n",
      "Epoch 14/100\n",
      "141/141 - 110s - loss: 0.9241 - val_loss: 1.4182\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.42090 to 1.41819, saving model to mtrans.h5\n",
      "Epoch 15/100\n",
      "141/141 - 107s - loss: 0.9025 - val_loss: 1.4155\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.41819 to 1.41549, saving model to mtrans.h5\n",
      "Epoch 16/100\n",
      "141/141 - 106s - loss: 0.8822 - val_loss: 1.4079\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.41549 to 1.40790, saving model to mtrans.h5\n",
      "Epoch 17/100\n",
      "141/141 - 108s - loss: 0.8631 - val_loss: 1.4153\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.40790\n",
      "Epoch 18/100\n",
      "141/141 - 107s - loss: 0.8435 - val_loss: 1.4045\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.40790 to 1.40448, saving model to mtrans.h5\n",
      "Epoch 19/100\n",
      "141/141 - 118s - loss: 0.8247 - val_loss: 1.3979\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.40448 to 1.39786, saving model to mtrans.h5\n",
      "Epoch 20/100\n",
      "141/141 - 127s - loss: 0.8010 - val_loss: 1.4001\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.39786\n",
      "Epoch 21/100\n",
      "141/141 - 127s - loss: 0.7777 - val_loss: 1.3972\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.39786 to 1.39719, saving model to mtrans.h5\n",
      "Epoch 22/100\n",
      "141/141 - 125s - loss: 0.7524 - val_loss: 1.3944\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.39719 to 1.39437, saving model to mtrans.h5\n",
      "Epoch 23/100\n",
      "141/141 - 124s - loss: 0.7289 - val_loss: 1.3967\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.39437\n",
      "Epoch 24/100\n",
      "141/141 - 126s - loss: 0.7042 - val_loss: 1.3935\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.39437 to 1.39349, saving model to mtrans.h5\n",
      "Epoch 25/100\n",
      "141/141 - 125s - loss: 0.6824 - val_loss: 1.3897\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.39349 to 1.38970, saving model to mtrans.h5\n",
      "Epoch 26/100\n",
      "141/141 - 126s - loss: 0.6565 - val_loss: 1.4000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.38970\n",
      "Epoch 27/100\n",
      "141/141 - 126s - loss: 0.6322 - val_loss: 1.3998\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.38970\n",
      "Epoch 28/100\n",
      "141/141 - 125s - loss: 0.6103 - val_loss: 1.4103\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.38970\n",
      "Epoch 29/100\n",
      "141/141 - 125s - loss: 0.5872 - val_loss: 1.4138\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.38970\n",
      "Epoch 30/100\n",
      "141/141 - 124s - loss: 0.5659 - val_loss: 1.4178\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.38970\n",
      "Epoch 31/100\n",
      "141/141 - 128s - loss: 0.5439 - val_loss: 1.4154\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.38970\n",
      "Epoch 32/100\n",
      "141/141 - 125s - loss: 0.5237 - val_loss: 1.4200\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.38970\n",
      "Epoch 33/100\n",
      "141/141 - 125s - loss: 0.5040 - val_loss: 1.4462\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.38970\n",
      "Epoch 34/100\n",
      "141/141 - 126s - loss: 0.4842 - val_loss: 1.4438\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.38970\n",
      "Epoch 35/100\n",
      "141/141 - 126s - loss: 0.4648 - val_loss: 1.4508\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.38970\n",
      "Epoch 36/100\n",
      "141/141 - 125s - loss: 0.4463 - val_loss: 1.4617\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.38970\n",
      "Epoch 37/100\n",
      "141/141 - 125s - loss: 0.4289 - val_loss: 1.4724\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.38970\n",
      "Epoch 38/100\n",
      "141/141 - 129s - loss: 0.4132 - val_loss: 1.4790\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.38970\n",
      "Epoch 39/100\n",
      "141/141 - 110s - loss: 0.3975 - val_loss: 1.4864\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.38970\n",
      "Epoch 40/100\n",
      "141/141 - 109s - loss: 0.3825 - val_loss: 1.5078\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.38970\n",
      "Epoch 41/100\n",
      "141/141 - 108s - loss: 0.3663 - val_loss: 1.5203\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.38970\n",
      "Epoch 42/100\n",
      "141/141 - 112s - loss: 0.3541 - val_loss: 1.5264\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.38970\n",
      "Epoch 43/100\n",
      "141/141 - 138s - loss: 0.3409 - val_loss: 1.5442\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.38970\n",
      "Epoch 44/100\n",
      "141/141 - 134s - loss: 0.3252 - val_loss: 1.5408\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.38970\n",
      "Epoch 45/100\n",
      "141/141 - 134s - loss: 0.3136 - val_loss: 1.5560\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.38970\n",
      "Epoch 46/100\n",
      "141/141 - 134s - loss: 0.3004 - val_loss: 1.5760\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.38970\n",
      "Epoch 47/100\n",
      "141/141 - 159s - loss: 0.2902 - val_loss: 1.5880\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.38970\n",
      "Epoch 48/100\n",
      "141/141 - 156s - loss: 0.2800 - val_loss: 1.5854\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.38970\n",
      "Epoch 49/100\n",
      "141/141 - 156s - loss: 0.2702 - val_loss: 1.6084\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.38970\n",
      "Epoch 50/100\n",
      "141/141 - 133s - loss: 0.2582 - val_loss: 1.6284\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.38970\n",
      "Epoch 51/100\n",
      "141/141 - 119s - loss: 0.2478 - val_loss: 1.6406\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.38970\n",
      "Epoch 52/100\n",
      "141/141 - 122s - loss: 0.2405 - val_loss: 1.6542\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.38970\n",
      "Epoch 53/100\n",
      "141/141 - 130s - loss: 0.2313 - val_loss: 1.6545\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.38970\n",
      "Epoch 54/100\n",
      "141/141 - 123s - loss: 0.2211 - val_loss: 1.6824\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.38970\n",
      "Epoch 55/100\n",
      "141/141 - 113s - loss: 0.2120 - val_loss: 1.6838\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.38970\n",
      "Epoch 56/100\n",
      "141/141 - 108s - loss: 0.2031 - val_loss: 1.6990\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.38970\n",
      "Epoch 57/100\n",
      "141/141 - 111s - loss: 0.1962 - val_loss: 1.7305\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.38970\n",
      "Epoch 58/100\n",
      "141/141 - 110s - loss: 0.1887 - val_loss: 1.7168\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.38970\n",
      "Epoch 59/100\n",
      "141/141 - 107s - loss: 0.1829 - val_loss: 1.7548\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.38970\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 - 113s - loss: 0.1764 - val_loss: 1.7552\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.38970\n",
      "Epoch 61/100\n",
      "141/141 - 109s - loss: 0.1699 - val_loss: 1.7542\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.38970\n",
      "Epoch 62/100\n",
      "141/141 - 111s - loss: 0.1625 - val_loss: 1.7794\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.38970\n",
      "Epoch 63/100\n",
      "141/141 - 107s - loss: 0.1548 - val_loss: 1.8001\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.38970\n",
      "Epoch 64/100\n",
      "141/141 - 106s - loss: 0.1491 - val_loss: 1.8118\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.38970\n",
      "Epoch 65/100\n",
      "141/141 - 110s - loss: 0.1450 - val_loss: 1.8077\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.38970\n",
      "Epoch 66/100\n",
      "141/141 - 110s - loss: 0.1412 - val_loss: 1.8366\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.38970\n",
      "Epoch 67/100\n",
      "141/141 - 107s - loss: 0.1399 - val_loss: 1.8314\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.38970\n",
      "Epoch 68/100\n",
      "141/141 - 113s - loss: 0.1327 - val_loss: 1.8595\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.38970\n",
      "Epoch 69/100\n",
      "141/141 - 119s - loss: 0.1257 - val_loss: 1.8725\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.38970\n",
      "Epoch 70/100\n",
      "141/141 - 114s - loss: 0.1213 - val_loss: 1.8785\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.38970\n",
      "Epoch 71/100\n",
      "141/141 - 119s - loss: 0.1170 - val_loss: 1.8686\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.38970\n",
      "Epoch 72/100\n",
      "141/141 - 114s - loss: 0.1104 - val_loss: 1.9126\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.38970\n",
      "Epoch 73/100\n",
      "141/141 - 110s - loss: 0.1074 - val_loss: 1.9218\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.38970\n",
      "Epoch 74/100\n",
      "141/141 - 113s - loss: 0.1051 - val_loss: 1.9276\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.38970\n",
      "Epoch 75/100\n",
      "141/141 - 110s - loss: 0.1004 - val_loss: 1.9393\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.38970\n",
      "Epoch 76/100\n",
      "141/141 - 61s - loss: 0.0953 - val_loss: 1.9596\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.38970\n",
      "Epoch 77/100\n",
      "141/141 - 61s - loss: 0.0906 - val_loss: 1.9755\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.38970\n",
      "Epoch 78/100\n",
      "141/141 - 67s - loss: 0.0877 - val_loss: 1.9749\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.38970\n",
      "Epoch 79/100\n",
      "141/141 - 71s - loss: 0.0849 - val_loss: 2.0160\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.38970\n",
      "Epoch 80/100\n",
      "141/141 - 105s - loss: 0.0828 - val_loss: 2.0035\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.38970\n",
      "Epoch 81/100\n",
      "141/141 - 108s - loss: 0.0809 - val_loss: 2.0502\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.38970\n",
      "Epoch 82/100\n",
      "141/141 - 109s - loss: 0.0795 - val_loss: 2.0346\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.38970\n",
      "Epoch 83/100\n",
      "141/141 - 109s - loss: 0.0817 - val_loss: 2.0279\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.38970\n",
      "Epoch 84/100\n",
      "141/141 - 109s - loss: 0.0798 - val_loss: 2.0631\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.38970\n",
      "Epoch 85/100\n",
      "141/141 - 109s - loss: 0.0752 - val_loss: 2.0630\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.38970\n",
      "Epoch 86/100\n",
      "141/141 - 109s - loss: 0.0696 - val_loss: 2.0574\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.38970\n",
      "Epoch 87/100\n",
      "141/141 - 110s - loss: 0.0654 - val_loss: 2.0715\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.38970\n",
      "Epoch 88/100\n",
      "141/141 - 109s - loss: 0.0604 - val_loss: 2.0916\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.38970\n",
      "Epoch 89/100\n",
      "141/141 - 110s - loss: 0.0579 - val_loss: 2.1098\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.38970\n",
      "Epoch 90/100\n",
      "141/141 - 111s - loss: 0.0593 - val_loss: 2.1232\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.38970\n",
      "Epoch 91/100\n",
      "141/141 - 110s - loss: 0.0628 - val_loss: 2.1340\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.38970\n",
      "Epoch 92/100\n",
      "141/141 - 109s - loss: 0.0639 - val_loss: 2.1118\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.38970\n",
      "Epoch 93/100\n",
      "141/141 - 109s - loss: 0.0608 - val_loss: 2.1495\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.38970\n",
      "Epoch 94/100\n",
      "141/141 - 110s - loss: 0.0578 - val_loss: 2.1510\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.38970\n",
      "Epoch 95/100\n",
      "141/141 - 110s - loss: 0.0539 - val_loss: 2.1718\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.38970\n",
      "Epoch 96/100\n",
      "141/141 - 110s - loss: 0.0513 - val_loss: 2.1749\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.38970\n",
      "Epoch 97/100\n",
      "141/141 - 109s - loss: 0.0515 - val_loss: 2.1669\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.38970\n",
      "Epoch 98/100\n",
      "141/141 - 110s - loss: 0.0493 - val_loss: 2.1969\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.38970\n",
      "Epoch 99/100\n",
      "141/141 - 110s - loss: 0.0496 - val_loss: 2.2018\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.38970\n",
      "Epoch 100/100\n",
      "141/141 - 110s - loss: 0.0475 - val_loss: 2.1870\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.38970\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import shuffle\n",
    "\n",
    "N_TOTAL_SENTENCES = 10000\n",
    "TRAINING_SIZE = 9000\n",
    "\n",
    "pairs = read_data()\n",
    "raw_dataset = array(pairs)\n",
    "dataset = raw_dataset[:N_TOTAL_SENTENCES]\n",
    "#shuffle(dataset)\n",
    "\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "src = dataset[:, 0]\n",
    "tgt = dataset[:, 1]\n",
    "\n",
    "src_train = train[:, 0]\n",
    "tgt_train = train[:, 1]\n",
    "\n",
    "src_test = test[:, 0]\n",
    "tgt_test = test[:, 1]\n",
    "\n",
    "src_tok = get_tokenizer(src)\n",
    "src_index = src_tok.word_index\n",
    "src_length = max_length(src)\n",
    "src_vocab_size = len(src_index) + 1\n",
    "\n",
    "tgt_tok = get_tokenizer(tgt)\n",
    "tgt_index = tgt_tok.word_index\n",
    "tgt_length = max_length(tgt)\n",
    "tgt_vocab_size = len(tgt_index) + 1\n",
    "\n",
    "trainX = encode_sequences(src_tok, src_length, src_train)\n",
    "trainY = encode_sequences(tgt_tok, tgt_length, tgt_train)\n",
    "trainY = encode_one_hot_sequences(trainY, tgt_vocab_size)\n",
    "\n",
    "testX = encode_sequences(src_tok, src_length, src_test)\n",
    "testY = encode_sequences(tgt_tok, tgt_length, tgt_test)\n",
    "testY = encode_one_hot_sequences(testY, tgt_vocab_size)\n",
    "\n",
    "model = define_model(src_vocab_size, tgt_vocab_size, src_length, tgt_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "print(model.summary())\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "filename='mtrans.h5'\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)\n",
    "model.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom's tired:[[112 188   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]:湯姆 累 了 。\n",
      "They hugged:[[  46 1029    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]:他们 拥抱 。\n",
      "hug me:[[1018   10    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]:为什么 是 。\n",
      "i:[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]:我 我 相信 相信 相信 相信 相信 进 进 进 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入\n",
      "Unbelievable!:[[1463    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]:难以置信\n",
      "i ran:[[  1 309   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]:我 喜欢 了 。\n",
      "Got it:[[69  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0]]:你 懂 了 吗 ？\n",
      "hug me:[[1018   10    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]:为什么 是 。\n",
      "i:[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]:我 我 相信 相信 相信 相信 相信 进 进 进 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入 入\n",
      "Wait!:[[157   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]:等 一下 ！\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from numpy import array\n",
    "\n",
    "def read_data(file):\n",
    "    pairs = []\n",
    "    f = open(file, mode='r', encoding ='utf8')\n",
    "    for x in f:\n",
    "        sample = json.loads(x)\n",
    "        pair = [sample['source'], sample['target']]\n",
    "        pairs.append(pair)\n",
    "    return array(pairs)\n",
    "\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from numpy import argmax\n",
    "\n",
    "model = load_model('mtrans.h5')\n",
    "myData = read_data('tiny.json')[:10]\n",
    "\n",
    "# from ipynb.fs.full.model import encode_sequences\n",
    "# from ipynb.fs.full.model import src_tok\n",
    "# from ipynb.fs.full.model import src_length\n",
    "# from ipynb.fs.full.model import tgt_tok\n",
    "\n",
    "myTest = encode_sequences(src_tok, src_length, myData[:,0])\n",
    "\n",
    "for count, source in enumerate(myTest):\n",
    "    source = source.reshape((1, source.shape[0]))\n",
    "    translation = predict_sequence(model, tgt_tok, source)\n",
    "    print(myData[:,0][count], source, translation, sep=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
